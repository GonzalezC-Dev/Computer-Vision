{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ee9866-f026-4d0e-a392-0c51fcb7158a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Training Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting SIFT features:  19%|██████████▏                                            | 92/495 [01:54<11:28,  1.71s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# --- Paths ---\n",
    "# Make sure these paths are correct for your system\n",
    "train_dir = r\"C:\\Users\\chris\\Downloads\\eclipse-megamovie\\train\"\n",
    "test_dir = r\"C:\\Users\\chris\\Downloads\\eclipse-megamovie\\test\"\n",
    "train_csv = r\"C:\\Users\\chris\\Downloads\\eclipse-megamovie\\train.csv\"\n",
    "test_csv = r\"C:\\Users\\chris\\Downloads\\eclipse-megamovie\\test.csv\"\n",
    "\n",
    "# --- Load label files ---\n",
    "train_df = pd.read_csv(train_csv)\n",
    "test_df = pd.read_csv(test_csv)\n",
    "\n",
    "# --- Add full paths ---\n",
    "train_df['path'] = train_df['image_id'].apply(lambda x: os.path.join(train_dir, x))\n",
    "test_df['path'] = test_df['image_id'].apply(lambda x: os.path.join(test_dir, x))\n",
    "\n",
    "# --- Preprocessing ---\n",
    "def preprocess_image(path, size=(256, 256)):\n",
    "    img = cv2.imread(path) # read image from file path\n",
    "    if img is None: # handle error image\n",
    "        print(f\"Warning: Could not read image {path}\")\n",
    "        return None\n",
    "    img = cv2.resize(img, size) # resize to 256x256\n",
    "    img = cv2.GaussianBlur(img, (3, 3), 0) # apply gaussian blur\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # convert image to grayscale\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8)) # create CLAHE\n",
    "    return clahe.apply(gray) # return preprocessed image, CLAHE applied\n",
    "\n",
    "# --- Special Case Classification ---\n",
    "def classify_flat_or_dark(img, flat_std_thresh=8, dark_mean_thresh=25, dark_std_thresh=6, debug=False, path=\"\"):\n",
    "    \"\"\" Classify an image as Flat (6), Dark (4), or Unknown (None). \"\"\"\n",
    "    if img is None:\n",
    "        return None\n",
    "\n",
    "    mean_intensity = np.mean(img)\n",
    "    std_intensity = np.std(img)\n",
    "    median_intensity = np.median(img)\n",
    "    hist = cv2.calcHist([img], [0], None, [256], [0, 256]).flatten()\n",
    "    dominant_pixel = np.argmax(hist)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] {path} -> Mean: {mean_intensity:.2f}, Std: {std_intensity:.2f}, Median: {median_intensity:.2f}, Dominant pixel: {dominant_pixel}\")\n",
    "        # (Optional: add histogram plotting code here if needed)\n",
    "\n",
    "    # Rule for Dark: very dim, low contrast, dominant near 0\n",
    "    if mean_intensity < dark_mean_thresh and std_intensity < dark_std_thresh and dominant_pixel < 20:\n",
    "        return 4  # Dark\n",
    "\n",
    "    # Rule for Flat: bright or mid-bright, but low variation, dominant peak not too dark\n",
    "    elif mean_intensity > 80 and std_intensity < flat_std_thresh and dominant_pixel > 50:\n",
    "        return 6  # Flat\n",
    "\n",
    "    return None # Otherwise unknown\n",
    "\n",
    "# --- SIFT - feature extract (No change needed here) ---\n",
    "def extract_sift_descriptors(df):\n",
    "    sift = cv2.SIFT_create(contrastThreshold=0.075, edgeThreshold=12)\n",
    "    descriptors_list = []\n",
    "    special_classifications = [] # Store special class (4 or 6) or None\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting SIFT features\"):\n",
    "        path = row['path']\n",
    "        img_orig = cv2.imread(path) # Read original for classification\n",
    "        if img_orig is None:\n",
    "            print(f\"Warning: Could not read original image {path} for special classification.\")\n",
    "            descriptors_list.append(None)\n",
    "            special_classifications.append(None)\n",
    "            continue\n",
    "\n",
    "        img_gray_orig = cv2.cvtColor(img_orig, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # --- Early classify as Flat or Dark based on original grayscale image ---\n",
    "        special_class = classify_flat_or_dark(img_gray_orig, debug=False, path=path)\n",
    "\n",
    "        if special_class is not None:\n",
    "            special_classifications.append(special_class) # Store 4 or 6\n",
    "            descriptors_list.append(None) # No descriptors needed\n",
    "            continue  # Skip SIFT extraction\n",
    "\n",
    "        # --- If not Flat/Dark, proceed with preprocessing and SIFT ---\n",
    "        special_classifications.append(None) # Mark as not special\n",
    "        gray_processed = preprocess_image(path) # Preprocess for SIFT\n",
    "\n",
    "        if gray_processed is None:\n",
    "            descriptors_list.append(None) # Preprocessing failed\n",
    "            continue\n",
    "\n",
    "        keypoints, descriptors = sift.detectAndCompute(gray_processed, None)\n",
    "\n",
    "        # Handle cases where no keypoints are found even if not Flat/Dark\n",
    "        if descriptors is None or len(descriptors) == 0:\n",
    "             descriptors_list.append(None)\n",
    "        else:\n",
    "            descriptors_list.append(descriptors)\n",
    "\n",
    "\n",
    "    return descriptors_list, special_classifications\n",
    "\n",
    "# --- KMeans Vocabulary Building (No change needed here) ---\n",
    "def build_vocabulary(descriptors_list_normal, vocab_size):\n",
    "    # Input should ONLY contain descriptors from non-Flat/Dark images\n",
    "    descriptors = [desc for desc in descriptors_list_normal if desc is not None]\n",
    "    if not descriptors:\n",
    "        raise ValueError(\"No valid descriptors found to build vocabulary. Check SIFT extraction and filtering.\")\n",
    "    descriptors_stacked = np.vstack(descriptors)\n",
    "    print(f\"Building vocabulary from {len(descriptors)} images ({descriptors_stacked.shape[0]} total descriptors)...\")\n",
    "    kmeans = KMeans(n_clusters=vocab_size, random_state=42, n_init=10) # n_init added for stability\n",
    "    kmeans.fit(descriptors_stacked)\n",
    "    return kmeans\n",
    "\n",
    "# --- Compute BoW Histograms (Slight adaptation for clarity/safety) ---\n",
    "def compute_bow_histograms_normal(descriptors_list_normal, kmeans):\n",
    "    # Input should ONLY contain descriptors from non-Flat/Dark images\n",
    "    # Assumes Nones are filtered *before* calling or handled if passed\n",
    "    vocab_size = kmeans.n_clusters\n",
    "    histograms = []\n",
    "\n",
    "    for descriptors in descriptors_list_normal:\n",
    "        if descriptors is None or len(descriptors) == 0:\n",
    "            # This case should ideally be minimized by filtering, but handle defensively\n",
    "            hist = np.zeros(vocab_size)\n",
    "        else:\n",
    "            words = kmeans.predict(descriptors)\n",
    "            hist, _ = np.histogram(words, bins=np.arange(vocab_size + 1))\n",
    "        histograms.append(hist)\n",
    "\n",
    "    return np.array(histograms)\n",
    "\n",
    "# --- Execute Feature Extraction ---\n",
    "print(\"Processing Training Data...\")\n",
    "train_descriptors_all, special_classifications_train = extract_sift_descriptors(train_df)\n",
    "print(\"Processing Test Data...\")\n",
    "test_descriptors_all, special_classifications_test = extract_sift_descriptors(test_df)\n",
    "\n",
    "# --- Filter Data: Separate Normal images from Flat/Dark ---\n",
    "\n",
    "# 1. Get Indices\n",
    "normal_train_indices = [i for i, sc in enumerate(special_classifications_train) if sc is None]\n",
    "special_train_indices = [i for i, sc in enumerate(special_classifications_train) if sc is not None]\n",
    "normal_test_indices = [i for i, sc in enumerate(special_classifications_test) if sc is None]\n",
    "special_test_indices = [i for i, sc in enumerate(special_classifications_test) if sc is not None]\n",
    "\n",
    "# 2. Filter Descriptors and Labels for NORMAL images (Handling potential None descriptors)\n",
    "train_descriptors_normal_valid = []\n",
    "y_train_normal_indices = [] # Keep track of original indices for labels\n",
    "for i in normal_train_indices:\n",
    "    if train_descriptors_all[i] is not None and len(train_descriptors_all[i]) > 0:\n",
    "        train_descriptors_normal_valid.append(train_descriptors_all[i])\n",
    "        y_train_normal_indices.append(i) # Store the original index\n",
    "\n",
    "test_descriptors_normal_valid = []\n",
    "y_test_normal_indices = [] # Keep track of original indices for combining later\n",
    "for i in normal_test_indices:\n",
    "     if test_descriptors_all[i] is not None and len(test_descriptors_all[i]) > 0:\n",
    "        test_descriptors_normal_valid.append(test_descriptors_all[i])\n",
    "        y_test_normal_indices.append(i) # Store the original index\n",
    "\n",
    "# 3. Get Labels for NORMAL training images\n",
    "y_train_normal = train_df['label'].values[y_train_normal_indices]\n",
    "\n",
    "print(f\"Training set: {len(train_df)} total, {len(special_train_indices)} special, {len(y_train_normal_indices)} normal with valid descriptors.\")\n",
    "print(f\"Test set: {len(test_df)} total, {len(special_test_indices)} special, {len(y_test_normal_indices)} normal with valid descriptors.\")\n",
    "\n",
    "if not y_train_normal_indices:\n",
    "    raise SystemExit(\"Error: No 'normal' images with valid descriptors found in the training set. Cannot proceed.\")\n",
    "\n",
    "\n",
    "# --- Build Vocabulary (using ONLY normal training images) ---\n",
    "vocab_size = 300\n",
    "kmeans = build_vocabulary(train_descriptors_normal_valid, vocab_size)\n",
    "\n",
    "# --- Compute BoW Histograms (using ONLY normal images) ---\n",
    "print(\"Computing BoW histograms for Normal training images...\")\n",
    "X_train_normal = compute_bow_histograms_normal(train_descriptors_normal_valid, kmeans)\n",
    "print(\"Computing BoW histograms for Normal test images...\")\n",
    "X_test_normal = compute_bow_histograms_normal(test_descriptors_normal_valid, kmeans)\n",
    "\n",
    "# --- Normalize Features (using ONLY normal images) ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_normal = scaler.fit_transform(X_train_normal)\n",
    "# Check if X_test_normal is empty before transforming\n",
    "if X_test_normal.shape[0] > 0:\n",
    "    X_test_scaled_normal = scaler.transform(X_test_normal)\n",
    "else:\n",
    "    # Handle case where there are no normal test images with descriptors\n",
    "    X_test_scaled_normal = np.empty((0, vocab_size))\n",
    "    print(\"Warning: No 'normal' test images with valid descriptors found.\")\n",
    "\n",
    "\n",
    "# --- Train Classifier (using ONLY normal images) ---\n",
    "print(\"Training SVM on Normal images...\")\n",
    "# === GRID SEARCH CV FOR SVM ===\n",
    "param_grid = {\n",
    "    'C': [1, 10, 100], # Reduced for potentially faster run, expand as needed\n",
    "    'gamma': [0.001, 0.01, 0.1, 1], # Reduced for potentially faster run\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "print(\"Running GridSearchCV...\")\n",
    "# Ensure there's data to train on\n",
    "if X_train_scaled_normal.shape[0] > 0:\n",
    "    grid = GridSearchCV(SVC(), param_grid, cv=min(3, X_train_scaled_normal.shape[0]), verbose=2, n_jobs=-1) # Adjust cv if dataset is small\n",
    "    grid.fit(X_train_scaled_normal, y_train_normal)\n",
    "    print(\"Best SVM Parameters:\", grid.best_params_)\n",
    "    svm = grid.best_estimator_\n",
    "else:\n",
    "    raise SystemExit(\"Error: Cannot train SVM, no valid normal training samples.\")\n",
    "\n",
    "\n",
    "# --- Predict with SVM (on ONLY normal test images) ---\n",
    "# Ensure there's data to predict on\n",
    "if X_test_scaled_normal.shape[0] > 0:\n",
    "    y_pred_normal = svm.predict(X_test_scaled_normal)\n",
    "else:\n",
    "    y_pred_normal = np.array([], dtype=int) # No predictions if no normal test images\n",
    "\n",
    "\n",
    "# --- Combine Predictions and Ground Truth ---\n",
    "\n",
    "# 1. Final Ground Truth (y_test_final)\n",
    "y_test_final = test_df['label'].values.copy() # Start with original labels\n",
    "for i in special_test_indices:\n",
    "    y_test_final[i] = special_classifications_test[i] # Override with special class\n",
    "\n",
    "# 2. Final Predictions (y_pred_final)\n",
    "y_pred_final = np.zeros(len(test_df), dtype=int) # Initialize\n",
    "\n",
    "# Fill in special classifications\n",
    "for i in special_test_indices:\n",
    "    y_pred_final[i] = special_classifications_test[i]\n",
    "\n",
    "# Fill in SVM predictions for normal images\n",
    "# Ensure the lengths match before assigning\n",
    "if len(y_pred_normal) == len(y_test_normal_indices):\n",
    "    for pred_idx, original_idx in enumerate(y_test_normal_indices):\n",
    "        y_pred_final[original_idx] = y_pred_normal[pred_idx]\n",
    "elif len(y_pred_normal) == 0 and len(y_test_normal_indices) == 0:\n",
    "     print(\"No normal test images to predict.\")\n",
    "else:\n",
    "    print(f\"Warning: Mismatch in length of normal predictions ({len(y_pred_normal)}) and normal indices ({len(y_test_normal_indices)}). Predictions might be incomplete.\")\n",
    "    # Handle potential mismatches cautiously if necessary, e.g., leave as 0 or assign a specific error code\n",
    "\n",
    "\n",
    "# --- Evaluate FINAL Results ---\n",
    "print(\"\\n--- Final Evaluation ---\")\n",
    "if len(y_test_final) > 0:\n",
    "    acc = accuracy_score(y_test_final, y_pred_final)\n",
    "    print(f\"Overall Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    # Dynamically get unique labels present in both true and pred for matrix plotting\n",
    "    labels = sorted(list(set(y_test_final) | set(y_pred_final)))\n",
    "    cm = confusion_matrix(y_test_final, y_pred_final, labels=labels)\n",
    "    df_cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.title(\"Final Confusion Matrix\\n(Includes Special Cases + SVM Predictions)\")\n",
    "    sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"YlGnBu\", cbar=True, linewidths=0.5)\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(y_test_final, y_pred_final, labels=labels, zero_division=0))\n",
    "else:\n",
    "    print(\"No test data to evaluate.\")\n",
    "\n",
    "\n",
    "# --- Visualize Misclassified Images (Using Final Results) ---\n",
    "if len(y_test_final) > 0:\n",
    "    misclassified_indices = np.where(y_pred_final != y_test_final)[0]\n",
    "    print(f\"\\nFound {len(misclassified_indices)} misclassified images.\")\n",
    "\n",
    "    # Number of misclassified images to show\n",
    "    num_to_show = min(12, len(misclassified_indices)) # Show up to 12 or fewer if less are misclassified\n",
    "    if num_to_show > 0:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plot_rows = (num_to_show + 3) // 4 # Calculate rows needed for a 4-column layout\n",
    "        for i, idx in enumerate(misclassified_indices[:num_to_show]):\n",
    "            path = test_df.iloc[idx]['path']\n",
    "            true_label = y_test_final[idx]\n",
    "            predicted_label = y_pred_final[idx]\n",
    "\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not read image {path} for misclassified visualization.\")\n",
    "                # Optionally plot a placeholder\n",
    "                ax = plt.subplot(plot_rows, 4, i + 1)\n",
    "                ax.set_title(f\"Idx: {idx}\\nTrue: {true_label}, Pred: {predicted_label}\\n(Image Read Error)\")\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            plt.subplot(plot_rows, 4, i + 1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"Idx: {idx}\\nTrue: {true_label}, Pred: {predicted_label}\")\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.suptitle(\"Misclassified Test Images (Final Results)\")\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# --- Visualize Histogram Comparison (Focus on NORMAL classes) ---\n",
    "# Find examples within the NORMAL, SCALED training data\n",
    "if X_train_scaled_normal.shape[0] > 0:\n",
    "    try:\n",
    "        # Find first index in the 'normal' set corresponding to a specific original label\n",
    "        partial_label_to_find = 3 # Example: 56–95% Partial Eclipse\n",
    "        # Find the first occurrence of this label *within the set used for training SVM*\n",
    "        target_indices_in_normal_set = np.where(y_train_normal == partial_label_to_find)[0]\n",
    "\n",
    "        if len(target_indices_in_normal_set) > 0:\n",
    "            partial_idx_in_normal = target_indices_in_normal_set[0]\n",
    "            partial_hist = X_train_scaled_normal[partial_idx_in_normal]\n",
    "\n",
    "            # Maybe compare two different 'normal' classes if available\n",
    "            other_label_to_find = 1 # Example: 1-25% Partial Eclipse\n",
    "            other_indices_in_normal_set = np.where(y_train_normal == other_label_to_find)[0]\n",
    "            other_hist = None\n",
    "            if len(other_indices_in_normal_set) > 0:\n",
    "                 other_idx_in_normal = other_indices_in_normal_set[0]\n",
    "                 other_hist = X_train_scaled_normal[other_idx_in_normal]\n",
    "\n",
    "\n",
    "            # Plot\n",
    "            plt.figure(figsize=(14, 5))\n",
    "            plt.plot(partial_hist, label=f\"Example Normal (label {partial_label_to_find})\", color='red')\n",
    "            if other_hist is not None:\n",
    "                 plt.plot(other_hist, label=f\"Example Normal (label {other_label_to_find})\", color='purple')\n",
    "\n",
    "            plt.title(\"BoW Scaled Histogram Comparison (Examples from Normal Set)\")\n",
    "            plt.xlabel(\"Visual Word Index (Scaled Feature Dimension)\")\n",
    "            plt.ylabel(\"Scaled Frequency\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Could not find example of label {partial_label_to_find} in the normal training set for histogram visualization.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate histogram comparison plot: {e}\")\n",
    "else:\n",
    "    print(\"Skipping histogram comparison: No normal training data available.\")\n",
    "\n",
    "print(\"\\nScript finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dda474-cd29-4ace-a8dd-f2dbd07e4d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
